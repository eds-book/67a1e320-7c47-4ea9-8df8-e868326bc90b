{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": "# Sea ice forecasting using the IceNet library"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Context\n",
    "### Purpose\n",
    "This notebook demonstrates the use of the [IceNet library](https://pypi.org/project/icenet/) for sea-ice forecasting trained using climate reanalysis and observational data.\n",
    "\n",
    "### Description\n",
    "[IceNet](https://github.com/icenet-ai/icenet/) is a Python library that provides the ability to download, process, train and predict from end to end. Users can interact with IceNet either via the Python interface or via a set of command-line interfaces (CLI) which provide a high-level interface that covers the above abilities.\n",
    "\n",
    "This notebook demonstrates the use of the Python library api for forecasting sea ice for a reduced dataset to demonstrate its capabilities. The final outputs of interest are maps of sea ice concentration.\n",
    "\n",
    "### Modelling approach\n",
    "IceNet is a probabilistic, deep learning sea ice forecasting system. It utilises ensemble modelling of [U-Net](https://link.springer.com/chapter/10.1007/978-3-319-24574-4_28) networks to generate daily forecasts of sea ice conditions, trained on climate reanalysis and sea ice observational data (the built-in downloaders within IceNet are extensible). The original IceNet research model, published in Nature Communications ([Seasonal Arctic sea ice forecasting with probabilistic deep learning](https://www.nature.com/articles/s41467-021-25257-4)) was trained on climate simulations and observational data to forecast the next 6 months of monthly-averaged sea ice concentration maps.\n",
    "\n",
    "The Python library ([IceNet](https://github.com/icenet-ai/icenet/)) showcased in this notebook is a heavily refactored version of the original research code from the original publication that has been developed for operational forecasting that utilises daily inputs and is able to generate daily forecast outputs for a variable forecast time period. The core U-Net architecture is implemented using Tensorflow, however, the library architecture allows other backend libraries to be utilised and wrapped around the IceNet library ecosystem.\n",
    "\n",
    "### Highlights\n",
    " * [1. Setup](##-1.-Setup) the environment and project structure.\n",
    " * [2. Download](##-2.-Download) sea ice concentration data as training data.\n",
    " * [3. Process](##-3.-Process) downloaded data, and generate cached datasets to speed up training.\n",
    " * [4. Train](##-4.-Train) the neural network and generate checkpoint and model output.\n",
    " * [5. Predict](##-5.-Predict) for defined dates.\n",
    " * [6. Visualisation](##-6.-Visualisation) of the prediction output.\n",
    "\n",
    "### Compatible platforms\n",
    "The `IceNet` library is designed to be run within a Unix environment, and, as such, this notebook has been tested on the following OS platforms:\n",
    "* Linux\n",
    "* macOS (Intel and Apple Silicon)\n",
    "\n",
    "Windows: although [WSL2](https://learn.microsoft.com/en-us/windows/wsl/install) should work, it is untested and is not a supported platform.\n",
    "\n",
    "In terms of hardware, this notebook (and IceNet in general) does not require a GPU, however, when attempting a larger training run, the use of a GPU is highly recommended.\n",
    "\n",
    ":::{seealso}\n",
    "__More in-depth notebooks on using IceNet are available [in this repository](https://github.com/icenet-ai/icenet-notebooks)__, including the use of ensemble modelling, and library extension to use different backends and neural network models.\n",
    "\n",
    "Contact us at either _bryald \\<at\\> bas.ac.uk_ or _jambyr \\<at\\> bas.ac.uk_ for anything else...\n",
    ":::\n",
    "\n",
    ":::{important}\n",
    "The original paper {cite:p}`Andersson2021` and notebook {cite:p}`cocacastro2024_icenetnb` used a combination of climate simulations and observational data to forecast the next 6 months of monthly-averaged sea ice concentration. Since then, the original code has been refactored into a new `icenet` library as showcased in this notebook.\n",
    "\n",
    "This library supports sea ice forecasting on a daily resolution rather than monthly-averaged. It has been developed significantly since the original paper to operationalise the code, and to that end, there are multiple ways of interacting with the library to help enable the development of sea ice forecasting and facilitate model development. More of these interfaces and use-case scenarios are covered in the [icenet-notebooks](https://github.com/icenet-ai/icenet-notebooks) repository.\n",
    ":::\n",
    "\n",
    "#### Source code\n",
    "There are multiple relevant code bases depending on the usage scenario, but the main Python IceNet library is located [here](https://github.com/icenet-ai/icenet). If of interest, other related repositories can be found in the [icenet-ai](https://github.com/icenet-ai/) github organisation.\n",
    "\n",
    "#### Involved organisations\n",
    "The Alan Turing Institute and the British Antarctic Survey\n",
    "\n",
    "### Acronyms\n",
    "\n",
    "<center>\n",
    "\n",
    "| Abbreviation | Definition    |\n",
    "|--------------|:--------------|\n",
    "| API          | Application Programming Interface |\n",
    "| CLI          | Command Line Interface |\n",
    "| ECMWF        | European Centre for Medium-Range Weather Forecasts |\n",
    "| [ERA5](https://www.ecmwf.int/en/forecasts/dataset/ecmwf-reanalysis-v5)    | ECMWF Reanalysis v5 |\n",
    "| [ORAS5](https://www.ecmwf.int/en/forecasts/dataset/ocean-reanalysis-system-5)    | Ocean Reanalysis System 5 |\n",
    "| [OSI SAF](https://osi-saf.eumetsat.int/) | Ocean and Sea Ice Satellite Application Facility |\n",
    "| SIC          | Sea Ice Concentration |\n",
    "\n",
    "</center>\n",
    "\n",
    "### References\n",
    "\n",
    "#### Software\n",
    "\n",
    "| Package     |      Citation      |\n",
    "|-------------|:--------------|\n",
    "| Cartopy | Met Office. (2010 - 2015). Cartopy: A cartographic python library with a Matplotlib interface. Exeter, Devon. Available from https://scitools.org.uk/cartopy. |\n",
    "| Dask | Dask Development Team (2016). Dask: Library for dynamic task scheduling. Available from http://dask.pydata.org |\n",
    "| Matplotlib  | J. D. Hunter, \"Matplotlib: A 2D Graphics Environment\", Computing in Science & Engineering, vol. 9, no. 3, pp. 90-95, 2007. [https://doi.org/10.1109/MCSE.2007.55](https://doi.org/10.1109/MCSE.2007.55) |\n",
    "| netCDF4-python | Jeff Whitaker, Constantine Khrulev, Filipe, David Huard, Stephan Hoyer, Mike Taves, Lars Pastewka, Alexander Mohr, Christian Marquardt, Bas Couwenberg, Christoph Paulik, Matthias Cuntz, Sander Roet, Jeffrey Whitaker, Matthew Brett, Max Bohnet, Miloššš Korenčiak, Rob Hetland, Andrew Barna, … bluppfisk. (2023). Unidata/netcdf4-python: version 1.6.0 release (Version v1.6.0rel) [Computer software]. Zenodo. [https://zenodo.org/doi/10.5281/zenodo.2592290](https://zenodo.org/doi/10.5281/zenodo.2592290) |\n",
    "| NumPy       | Harris, C.R., Millman, K.J., van der Walt, S.J. et al. Array programming with NumPy. Nature 585, 357–362 (2020). DOI: [10.1038/s41586-020-2649-2](https://doi.org/10.1038/s41586-020-2649-2). (Publisher link).      |\n",
    "| Pandas      | pandas development team, T. (2020). pandas-dev/pandas: Pandas (latest) [Computer software]. Zenodo. [https://doi.org/10.5281/zenodo.3509134](https://doi.org/10.5281/zenodo.3509134) |\n",
    "| seaborn     | Waskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021, [https://doi.org/10.21105/joss.03021](https://doi.org/10.21105/joss.03021). |\n",
    "| TensorFlow  | Martín Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving, Michael Isard, Rafal Jozefowicz, Yangqing Jia, Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan Mané, Mike Schuster, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015. Software available from tensorflow.org. |\n",
    "| xarray      | Hoyer, S. & Hamman, J., (2017). xarray: N-D labeled Arrays and Datasets in Python. Journal of Open Research Software. 5(1), p.10. DOI: [https://doi.org/10.5334/jors.148](https://doi.org/10.5334/jors.148) |\n",
    "\n",
    "#### Data Sources\n",
    "The data sources used in this notebook rely on the following:\n",
    "* [ERA5](https://doi.org/10.24381/cds.adbb2d47):\n",
    "    Hersbach, H., Bell, B., Berrisford, P., Biavati, G., Horányi, A., Muñoz Sabater, J., Nicolas, J., Peubey, C., Radu, R., Rozum, I., Schepers, D., Simmons, A., Soci, C., Dee, D., Thépaut, J-N. (2023): ERA5 hourly data on single levels from 1940 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS), DOI: [10.24381/cds.adbb2d47](https://doi.org/10.24381/cds.adbb2d47) (Accessed on 02-JUN-2024)\n",
    "* [ORAS5](https://doi.org/10.48670/moi-00024):\n",
    "    Generated using E.U. Copernicus Marine Service Information;. DOI: [10.48670/moi-00024](https://doi.org/10.48670/moi-00024) (Accessed on 07-JUN-2024)\n",
    "* [OSI SAF](http://dx.doi.org/10.15770/EUM_SAF_OSI_0013):\n",
    "    OSI SAF Global sea ice concentration climate data record 1978-2020 (v3.0, 2022), OSI-450-a, doi:[10.15770/EUM_SAF_OSI_0013](http://dx.doi.org/10.15770/EUM_SAF_OSI_0013). EUMETSAT Ocean and Sea Ice Satellite Application Facility. Data extracted from OSI SAF FTP server: (01-JAN-2020 to 30-APR-2020) accessed 02-Jun-2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "Load some of the common libraries required."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# We also set the logging level so that we get some feedback from the API\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set up logging level\n",
    "import dask\n",
    "import distributed\n",
    "import xarray\n",
    "logging.getLogger('icenet').setLevel(logging.INFO)\n",
    "logging.getLogger('dask').setLevel(logging.ERROR)\n",
    "logging.getLogger('distributed').setLevel(logging.ERROR)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following imports modules from the IceNet library as preparation for the downloaders. Whose instantiation describes the interactions with the upstream APIs/data interfaces used to source various types of data. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from icenet.data.sic.mask import Masks\n",
    "from icenet.data.sic.osisaf import SICDownloader"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%env PYTHONWARNINGS=ignore"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "___\n",
    "## 2. Download\n",
    "\n",
    "In this section, we download all required data with our extended date range. All downloaders inherit a `download` method from the `Downloader` class in [`icenet.data.producers`](https://github.com/icenet-ai/icenet/blob/main/icenet/data/producers.py), which also contains two other data producing classes `Generator` (which Masks inherits from) and `Processor` (used in the next section), each providing abstract implementations that multiple classes derive from."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Masked regions\n",
    "\n",
    "We start here with generating the masks for training/prediction. This includes regions where sea ice does not form such as land regions, and also, the unobserved [polar hole](https://blogs.egu.eu/divisions/cr/2016/10/14/image-of-the-week-the-polar-hole/) region.\n",
    "\n",
    ":::note\n",
    "This data is downloaded from OSI SAF over an FTP server which is blocked and not accessible via Binder, hence, the final datafile that the next cell would output for this range of dates is included in the repository.\n",
    "\n",
    "It does not change the workflow or the code, the class will simply skip over the date ranges that have already been downloaded.\n",
    ":::"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "masks = Masks(north=False, south=True)\n",
    "masks.generate(save_polarhole_masks=False)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Climate and Ocean data"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Climate and ocean data can be obtained from the [Climate Data Store (CDS)](https://cds.climate.copernicus.eu/). Due to resource limitations, we omit their use in this notebook. However, this section gives an example of how they could be downloaded.\n",
    "\n",
    "* The climate data used for training is from [ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview) reanalysis which covers the global climate from 1940 to the present time.\n",
    "\n",
    "* Ocean data can be downloaded from an ensemble of models, including [ORAS5](https://data.marine.copernicus.eu/product/GLOBAL_MULTIYEAR_PHY_ENS_001_031/description) which also uses a reanalysis approach and contains global ocean and sea-ice reanalysis data.\n",
    "\n",
    "Since these are both obtained from reanalysis, they are a combination of physical models and observational data. Due to the reanalysis approach, there is no temporal or spatial gap in the downloaded data. Both of these sets of data are obtained from the ECMWF's (European Centre for Medium-Range Weather Forecast) reanalysis systems.\n",
    "\n",
    "Please see the above links for more details on these datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ERA5Downloader` inherits from `ClimateDownloader`, from which several implementations derive their functionality. Two particularly useful methods shown below allow the downloaded data to be converted to the same grid and orientation as the OSISAF sea-ice concentration (SIC) data in the next cell.\n",
    "\n",
    "```python\n",
    "era5.regrid()                                   # Map data onto common EASE2 grid\n",
    "era5.rotate_wind_data()                         # Rotate wind data to correct orientation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ORAS5 Downloader\n",
    "\n",
    "The ORAS5 Downloader uses the Copernicus Marine toolbox which requires [registration](https://data.marine.copernicus.eu/register).\n",
    "\n",
    "<details close>\n",
    "<summary>Once registered, please follow these steps to set up credentials</summary>\n",
    "<br>\n",
    "\n",
    "**Option 1:**\n",
    "\n",
    "By defining a `~/.cmems.creds` file (i.e. in your home directory) as follows:\n",
    "\n",
    "```\n",
    "[auth]\n",
    "username = my_copernicus_marine_username\n",
    "password = my_copernicus_marine_password\n",
    "```\n",
    "\n",
    "**Option 2:**\n",
    "\n",
    "By setting the following environment variables:\n",
    "\n",
    "```bash\n",
    "export COPERNICUSMARINE_SERVICE_USERNAME=my_copernicus_marine_username\n",
    "export COPERNICUSMARINE_SERVICE_PASSWORD=my_copernicus_marine_password\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Once configured, the ORAS5Downloader can used as follows:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from icenet.data.interfaces.cmems import ORAS5Downloader\n",
    "\n",
    "vars = [\"thetao\"]\n",
    "\n",
    "oras5 = ORAS5Downloader(\n",
    "    var_names=vars,                             # Name of variables to download\n",
    "    dates=[                                     # Dates to download the variable data for\n",
    "        pd.to_datetime(date).date()\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")\n",
    "    ],\n",
    "    path=\"./data\",                              # Location to download data to (default is `./data`)\n",
    "    delete_tempfiles=True,                      # Whether to delete temporary downloaded files\n",
    "    levels=[None for _ in vars],                # A list of None for number of variables\n",
    "    max_threads=1,                              # Maximum number of concurrent downloads\n",
    "    north=False,                                # Boolean: Whether require data across northern hemisphere\n",
    "    south=True)                                 # Boolean: Whether require data across southern hemisphere\n",
    "oras5.download()                                # Start downloading\n",
    "oras5.regrid()\n",
    "```\n",
    "\n",
    "The following variables are available via the downloader. These also follow the CMIP convention which can be referenced [here](https://clipc-services.ceda.ac.uk/dreq/mipVars.html); or, via the original dataset's manual found [here](https://catalogue.marine.copernicus.eu/documents/PUM/CMEMS-GLO-PUM-001-031.pdf), in which case, the variable name ends with `_oras`, e.g. search for `thetao_oras`.\n",
    "\n",
    "<center>\n",
    "\n",
    "| Variable name | Definition                                      | Units |\n",
    "|---------------|:------------------------------------------------|-------|\n",
    "| thetao        | Sea Water Potential Temperature                 | degC  |\n",
    "| so            | Sea Water Salinity                              | PSU   |\n",
    "| uo            | Sea Water Eastward Velocity                     | m/s   |\n",
    "| vo            | Sea Water Northward Velocity                    | m/s   |\n",
    "| zos           | Sea Surface Height Above Geoid                  | m     |\n",
    "| mlotst        | Ocean Mixed Layer Thickness Defined by Sigma T  | m     |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Sea-ice concentration (SIC) data"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The sea-ice concentration data used for training is obtained from [OSI SAF](https://osi-saf.eumetsat.int/products/sea-ice-products).\n",
    "\n",
    "The SIC is defined as the fraction of a grid cell that is covered in sea-ice.\n",
    "\n",
    "You will notice a familiar interface with the `ERA5Downloader` class with the `SICDownloader` class, with similar input arguments.\n",
    "\n",
    ":::{note}\n",
    "In a similar manner to the Masks data download above, the OSI SAF data is downloaded over an FTP server which is blocked and not accessible via Binder, hence, the final datafile that the next cell would output for this range of dates is included in the repository.\n",
    "\n",
    "It does not change the workflow or the code, the class will simply skip over the date ranges that have already been downloaded.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sic = SICDownloader(\n",
    "    dates=[\n",
    "        pd.to_datetime(date).date()     # Dates to download the variable data for\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")\n",
    "    ],\n",
    "    delete_tempfiles=True,              # Whether to delete temporary downloaded files\n",
    "    north=False,                        # Boolean: Whether to use mask for this region\n",
    "    south=True,                         # Boolean: Whether to use mask for this region\n",
    "    parallel_opens=True,                # Boolean: Whether to use `dask.delayed` to open and preprocess multiple files in parallel\n",
    ")\n",
    "\n",
    "sic.download()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Downloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are also other downloaders available within IceNet that may also be of interest. These can also be used for training the model, and not just sea ice concentration or climate reanalysis data (ERA5).\n",
    "\n",
    "1. The [CMIP6](https://pcmdi.llnl.gov/CMIP6/) climate simulation data can be downloaded using:\n",
    "\n",
    "    ```python\n",
    "    from icenet.data.interfaces.esgf import CMIP6Downloader\n",
    "    ```\n",
    "\n",
    "1. In addition to ERA5 for downloading climate reanalysis data (shown above), ocean reanalysis data ([ORAS5](https://www.ecmwf.int/en/forecasts/dataset/ocean-reanalysis-system-5)) can be downloaded via:\n",
    "    ```python\n",
    "    from icenet.data.interfaces.cmems import ORAS5Downloader\n",
    "    ```\n",
    "\n",
    "1. Data from ECMWF High Resolution Forecast ([HRES](https://www.ecmwf.int/en/forecasts/datasets/catalogue-ecmwf-real-time-products)) can be downloaded using:\n",
    "    ```python\n",
    "    from icenet.data.interfaces.mars import HRESDownloader\n",
    "    ```\n",
    "\n",
    "1. Data from ECMWF Long-Range Seasonal Forecast ([SEAS](https://gmd.copernicus.org/articles/12/1087/2019/)) can be downloaded using:\n",
    "    ```python\n",
    "    from icenet.data.interfaces.mars import SEASDownloader\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 3. Process\n",
    "\n",
    "Similarly to the downloaders, each data producer (be it a `Downloader` or `Generator`) has a respective `Processor` that converts the `./data/` products into a normalised, preprocessed dataset under `./processed/`.\n",
    "\n",
    "Firstly, to make life a bit easier, we set up some variables. In this case, we're creating a train/validate/test split out of the 2020 data in a fairly naive manner."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "processing_dates = dict(\n",
    "    train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-01-04\")],\n",
    "    val=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-03\", \"2020-04-06\")],\n",
    "    test=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    ")\n",
    "processed_name = \"notebook_api_data\""
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the data producer and configure them for the dataset we want to create."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These modules import the Processing modules for the downloaded data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from icenet.data.processors.era5 import IceNetERA5PreProcessor # Unused in this demonstrator notebook\n",
    "from icenet.data.processors.meta import IceNetMetaPreProcessor\n",
    "from icenet.data.processors.osi import IceNetOSIPreProcessor"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "osi = IceNetOSIPreProcessor(\n",
    "    [\"siconca\"],                # Absolute normalised variables\n",
    "    [],                         # Variables defined as deviations from an aggregated norm\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True,\n",
    ")\n",
    "\n",
    "meta = IceNetMetaPreProcessor(\n",
    "    processed_name,\n",
    "    north=False,\n",
    "    south=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, this demonstrator does not use the ERA5 climate reanalysis data since a private API key should be set up by the user to access the CDS API ([how-to here](https://cds.climate.copernicus.eu/api-how-to)). However, once set up, the ERA5 data can also be preprocessed by:\n",
    "\n",
    "```python\n",
    "pp = IceNetERA5PreProcessor(\n",
    "    [\"uas\", \"vas\"],             # Absolute normalised variables\n",
    "    [\"tas\", \"zg500\", \"zg250\"],  # Variables defined as deviations from an aggregated norm\n",
    "    processed_name,\n",
    "    processing_dates[\"train\"],\n",
    "    processing_dates[\"val\"],\n",
    "    processing_dates[\"test\"],\n",
    "    linear_trends=tuple(),\n",
    "    north=False,\n",
    "    south=True\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialise the data processors using `init_source_data` which scans the data source directories to understand what data is available for processing based on the parameters. Since we named the processed data `\"notebook_api_data\"` above, it will create a data loader config file, `loader.notebook_api_data.json`, in the current directory.\n",
    "\n",
    "If we downloaded `ERA5` data, it could also be preprocessed via `pp.process()`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "osi.init_source_data(\n",
    "    lag_days=1,\n",
    ")\n",
    "osi.process()\n",
    "\n",
    "meta.process()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    ":::{note}\n",
    "Since both the ERA5 and ORAS5 data downloads require registration before download, this demonstrator will only download and use observed sea ice concentration data for training.\n",
    ":::\n",
    "\n",
    "### ERA5 Downloader\n",
    "\n",
    "The downloader implementation of this data in IceNet utilises the CDS API which requires registration and configuration of an API key before downloading. The registration is free, please see [the official CDS API how-to](https://cds.climate.copernicus.eu/api-how-to) for more instructions on how to set this up.\n",
    "\n",
    "Once the key is configured correctly, you can utilise the `ERA5Downloader` class, for example, to download climate variables and use it for training the model in addition to the sea ice concentration data that is downloaded further below.\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from icenet.data.interfaces.cds import ERA5Downloader\n",
    "\n",
    "era5 = ERA5Downloader(\n",
    "    var_names=[\"tas\", \"zg\", \"uas\", \"vas\"],      # Name of variables to download\n",
    "    dates=[                                     # Dates to download the variable data for\n",
    "        pd.to_datetime(date).date()\n",
    "        for date in pd.date_range(\"2020-01-01\", \"2020-04-30\", freq=\"D\")\n",
    "    ],\n",
    "    path=\"./data\",                              # Location to download data to (default is `./data`)\n",
    "    delete_tempfiles=True,                      # Whether to delete temporary downloaded files\n",
    "    levels=[None, [250, 500], None, None],      # The levels at which to obtain the variables for (e.g. for zg, it is the pressure levels)\n",
    "    max_threads=4,                              # Maximum number of concurrent downloads\n",
    "    north=False,                                # Boolean: Whether require data across northern hemisphere\n",
    "    south=True,                                 # Boolean: Whether require data across southern hemisphere\n",
    "    use_toolbox=False)                          # Experimental, alternative download method\n",
    "era5.download()                                 # Start downloading\n",
    "```\n",
    "\n",
    "A reference for understanding the variable names used above can be found [here](https://clipc-services.ceda.ac.uk/dreq/mipVars.html). These variable names are mapped to their equivalent ERA5 names within IceNet. For example, `uas` and `vas` are near-surface eastward and northward wind speed components in `m/s`. The variable mapping within Icenet for ERA5Downloader are:\n",
    "\n",
    "<center>\n",
    "\n",
    "| Variable name | Definition                              | Units |\n",
    "|---------------|:-------------------------------------   |-------|\n",
    "| tas           | Near Surface Air Temperature (at 2m)    | K     |\n",
    "| ta            | Air Temperature                         | K     |\n",
    "| tos           | Sea Surface Temperature                 | K     |\n",
    "| psl           | Sea Level Pressure at mean sea level    | Pa    |\n",
    "| zg            | Geopotential Height                     | m     |\n",
    "| hus           | Specific Humidity                       | -     |\n",
    "| rlds          | Surface Downwelling Longwave Radiation  | W/m^2 |\n",
    "| rsds          | Surface Downwelling Shortwave Radiation | W/m^2 |\n",
    "| uas           | Eastward Near-Surface Wind at 10m       | m/s   |\n",
    "| vas           | Eastward Near-Surface Wind at 10m       | m/s   |\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point the preprocessed data is ready to convert or create a configuration for the network dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset creation\n",
    "\n",
    "Now, we can create a dataset configuration for training the network. This can include cached data for the network in the format of a TFRecordDataset compatible set of tfrecords. To achieve this we create the `IceNetDataLoader`, which can both generate `IceNetDataSet` configurations (which easily provide the necessary functionality for training and prediction) as well as individual data samples for direct usage."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from icenet.data.loaders import IceNetDataLoaderFactory\n",
    "\n",
    "implementation = \"dask\"\n",
    "loader_config = f\"loader.{processed_name}.json\"\n",
    "dataset_name = \"api_dataset\"\n",
    "lag = 1\n",
    "\n",
    "dl = IceNetDataLoaderFactory().create_data_loader(\n",
    "    implementation,\n",
    "    loader_config,\n",
    "    dataset_name,\n",
    "    lag,\n",
    "    n_forecast_days=3,\n",
    "    north=False,\n",
    "    south=True,\n",
    "    output_batch_size=1,\n",
    "    generate_workers=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the loader config contains information about the data sources included and also the different dates to use for the training, validation and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dl._config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we can either use `generate` or `write_dataset_config_only` to produce a ready-to-go `IceNetDataSet` configuration. Both of these will generate a dataset config, `dataset_config.api_dataset.json` (recall we set the dataset name as `api_dataset` above)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "dl.generate()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate samples from this dataset, we can use the `.generate_sample()` method, which returns the inputs `x`, `y` and sample weights `sw`:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "x, y, sw = dl.generate_sample(pd.Timestamp(\"2020-04-01\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(f\"type(x): {type(x)}, x.shape: {x.shape}\")\n",
    "print(f\"type(y): {type(y)}, y.shape: {y.shape}\")\n",
    "print(f\"type(sw): {type(sw)}, sw.shape: {sw.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 4. Train\n",
    "\n",
    "For single runs, we can programmatically call `train_model` which defines the training process from start to finish. If we wanted to run multiple models with different initialisation states (called ensemble modelling) to obtain some level of model uncertainty, it is recommended to use the [`model-ensembler`](https://github.com/JimCircadian/model-ensembler) tool which works outside of the API and is capable of controlling multiple CLI submissions. Customising an ensemble can be achieved by looking at the configuration in [the pipeline repository](https://github.com/icenet-ai/icenet-pipeline?tab=readme-ov-file#configuration). That said, if workflow system integration (e.g. [Airflow](https://airflow.apache.org/)) is desired, integrating via this method is the way to go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load the dataset configuration by creating a `IceNetDataSet` from the `dataset_config.api_dataset.json` file and using the `_config` method."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from icenet.data.dataset import IceNetDataSet\n",
    "\n",
    "dataset_config = f\"dataset_config.{dataset_name}.json\"\n",
    "dataset = IceNetDataSet(dataset_config, batch_size=1)\n",
    "strategy = tf.distribute.get_strategy()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can view the loaded dataset configuration."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset._config"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can obtain the data loader that was used to create the dataset config via the `.get_data_loader()` method:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "dataset.get_data_loader()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use `train_model` function to train the U-Net model based on the above downloaded and processed data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "%%time\n",
    "from icenet.model.train import train_model\n",
    "\n",
    "run_name = \"api_test_run\"\n",
    "seed = 42\n",
    "\n",
    "trained_path, history = train_model(\n",
    "    run_name=run_name,\n",
    "    dataset=dataset,\n",
    "    epochs=3,              # Number of epochs to run\n",
    "    n_filters_factor=0.05, # Scale size of neural network\n",
    "    seed=seed,             # Random seed\n",
    "    strategy=strategy,     # Tensorflow's training strategy\n",
    "    training_verbosity=2,\n",
    "    workers=1\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen above, the training workflow is very standard for deep learning networks, with `train_model` wrapping up the training process with a lot of customisation of extraneous functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 5. Predict\n",
    "\n",
    "In much the same manner as with `train_model`, the `predict_forecast` method acts as a convenient entry point workflow system integration, CLI entry as well as an overridable method upon which to base custom implementations. Using the method directly relies on loading from a prepared (but perhaps not cached) dataset.\n",
    "\n",
    "Some parameters are fed to `predict_forecast` that ideally shouldn't need to be specified (like `seed` and `n_filters_factor` (which can be used to scale the size of the neural network)) and might seem contextually odd. They're used to locate the appropriate saved network. *This will be cleaned up in a future version of IceNet*."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%%time\n",
    "from icenet.model.predict import predict_forecast\n",
    "\n",
    "# Follows the naming convention used by the CLI version\n",
    "output_dir = os.path.join(\".\", \"results\", \"predict\",\n",
    "                          \"custom_run_forecast\",\n",
    "                          \"{}.{}\".format(run_name, \"42\"))\n",
    "\n",
    "predict_forecast(\n",
    "    dataset_config=dataset_config,\n",
    "    network_name=run_name,\n",
    "    n_filters_factor=0.05,\n",
    "    output_folder=output_dir,\n",
    "    seed=seed,\n",
    "    start_dates=[pd.to_datetime(el).date()\n",
    "                 for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    "    test_set=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The individual outputs of the above command are deposited into the following directory:\n",
    "\n",
    "```bash\n",
    "results/predict/custom_run_forecast/api_test_run.42/\n",
    "├── 2020_04_01.npy\n",
    "└── 2020_04_02.npy\n",
    "```\n",
    "\n",
    "The persistence and respective use of these results is then up to the user. They consist of NumPy (*.npy) files for each test date that contain the following:\n",
    "1. *forecast*: The neural network predicted forecast sea ice concentration data.\n",
    "1. *outputs*: Outputs from the data loader used for training.\n",
    "1. *sample_weights*: The sample weights used to weight training samples.\n",
    "\n",
    "To generate __a CF-compliant NetCDF containing the forecasts requested__ we need to run `icenet_output`, these can then be post-processed. As an input to this command, we need to provide it with a `csv` containing the test dates. In this example, we use `printf` to generate the required file."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!printf \"2020-04-01\\n2020-04-02\" | tee predict_dates.csv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "!icenet_output -m -o ./results/predict custom_run_forecast api_dataset predict_dates.csv"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## 6. Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that we have a prediction, we can visualise the binary sea ice concentration using some of the built-in tools in IceNet that utilise `cartopy` and `matplotlib`.\n",
    "\n",
    ":::{note}\n",
    "There are also some scripts in the [icenet-pipeline](https://github.com/icenet-ai/icenet-pipeline) repository that enable plotting common results such as `produce_op_assets.sh`)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are loading the prediction netCDF file we've just created in the previous step.\n",
    "\n",
    "We are also using the `Masks` class from IceNet to create a land mask region that will mask out the land regions in the forecast plot."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from icenet.plotting.video import xarray_to_video as xvid\n",
    "from icenet.data.sic.mask import Masks\n",
    "from IPython.display import HTML, display\n",
    "import xarray as xr, pandas as pd, datetime as dt\n",
    "\n",
    "def plot_result(file_path):\n",
    "    # Load our output prediction file\n",
    "    ds = xr.open_dataset(file_path)\n",
    "\n",
    "    # Get land mask to mask these regions in final plot\n",
    "    land_mask = Masks(south=True, north=False).get_land_mask()\n",
    "\n",
    "    # We obtain the start date of the forecast we would like to plot\n",
    "    forecast_date = ds.time.values[0]\n",
    "    print(forecast_date)\n",
    "\n",
    "    # The next line conducts the following operations:\n",
    "    # 1. Select the first time slice of the `sic_mean` variable\n",
    "    # 2. Drops the `time` variable from the dataset\n",
    "    # 3. Renames the variable `leadtime` to `time`\n",
    "    # The `leadtime` column contains forecast day as an integer, i.e. [1, 2, 3].\n",
    "    fc = ds.sic_mean.isel(time=0).drop_vars(\"time\").rename(dict(leadtime=\"time\"))\n",
    "\n",
    "    # We update the `time` column with the actual dates,\n",
    "    # starting from the forecast date to the forecast end date.\n",
    "    # E.g. [2020-04-02..., ..., ...]\n",
    "    fc['time'] = [pd.to_datetime(forecast_date) \\\n",
    "                + dt.timedelta(days=int(e)) for e in fc.time.values]\n",
    "\n",
    "    anim = xvid(fc, 15, figsize=4, mask=land_mask, colorbar_label=\"Sea-ice concentration fraction\")\n",
    "    display(HTML(anim.to_jshtml()))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, we can use the built in plotting tool to visualise our forecast.\n",
    "\n",
    ":::{note}\n",
    "Since this is a demonstrator notebook and due to the resource limitations on Binder, we have not trained our network for a prolonged period of time or for a large date range, but the plot below shows indicative results of binary sea ice concentration.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_result(\"results/predict/custom_run_forecast.nc\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "___\n",
    "### Improving results\n",
    "\n",
    "If we make the following changes above to increase the range of data we provide for training and validation, the number of epochs we train for, and the scale of the neural network, we can get a better result. These have been restricted in this notebook to keep within Binder's resource limits.\n",
    "\n",
    "1. **Increase training and validation date range.**\n",
    "\n",
    "    ```python\n",
    "    processing_dates = dict(\n",
    "        train=[pd.to_datetime(el) for el in pd.date_range(\"2020-01-01\", \"2020-03-31\")], # <-- Increase to 91 days for training\n",
    "        val=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-03\", \"2020-04-20\")],   # <-- Increase to 17 days for validation\n",
    "        test=[pd.to_datetime(el) for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    "    )\n",
    "    ```\n",
    "\n",
    "2. **Increase training epochs.**\n",
    "\n",
    "3. **Increase neural network size by increasing n_filters_factor in both train and predict.**\n",
    "\n",
    "    ```python\n",
    "    trained_path, history = train_model(\n",
    "        run_name=run_name,\n",
    "        dataset=dataset,\n",
    "        epochs=20,             # <-- Increase training epochs to 20\n",
    "        n_filters_factor=0.3,  # <-- Increase neural network size by scaling to 0.3 from 0.05\n",
    "        seed=seed,\n",
    "        strategy=strategy,\n",
    "        training_verbosity=2,\n",
    "        workers=1\n",
    "    )\n",
    "\n",
    "    predict_forecast(\n",
    "        dataset_config=dataset_config,\n",
    "        network_name=run_name,\n",
    "        n_filters_factor=0.3,  # <-- Increase to 0.3 to match training update\n",
    "        output_folder=output_dir,\n",
    "        seed=seed,\n",
    "        start_dates=[pd.to_datetime(el).date()\n",
    "                    for el in pd.date_range(\"2020-04-01\", \"2020-04-02\")],\n",
    "        test_set=True,\n",
    "    )\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is the result after making the above changes.\n",
    "\n",
    ":::{note}\n",
    "Since the training runs are non-deterministic (when run on GPUs, seeds do not always define determinism), the results you obtain when running with the same parameters are not necessarily the same, or at the very least, not an exact match.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "plot_result(\"results/predict/good_forecast.nc\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the use of the IceNet library to programmatically generate sea-ice forecasts via the `IceNet` Python interface, across all stages of a machine learning workflow, from source data download to forecast visualisation. It demonstrated this by:\n",
    "\n",
    "* Showing usage of the IceNet library to download different climate data variables ([ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=overview)) and sea ice concentration maps ([OSI SAF](https://osi-saf.eumetsat.int/)) from different data sources via a similar API interface. (The ERA5 code was shown, but not run due to the need for a free personal API key having been setup by the user)\n",
    "* Processing of downloaded data to include normalisation and data caching to speed up the training.\n",
    "* Showing the use of a high-level interface to the IceNet U-Net model to train on different data sources and generate predictions of binary sea ice concentration.\n",
    "* Visualising the predictions using the plotting tools within the IceNet library.\n",
    "* Visualising good results with increased network size and runtime.\n",
    "* If researching, consider [extending the functionality of the API to include revised or completely new implementations, such as additional data sources](https://github.com/icenet-ai/icenet-notebooks/blob/main/05.library_extension.ipynb)\n",
    "\n",
    "## Limitations\n",
    "\n",
    "For the training model generated, the accuracy of the results depends on the data used as training inputs (in this demonstrator notebook, only a limited subset of the OSI SAF SIC data is used due to resource limitations on Binder).\n",
    "\n",
    "On less resource-constricted systems, as elucidated above, it is possible to use other data downloaders that include variables which relate to sea ice formation to improve prediction results while including a larger date range as available from the different data sources. For day to day forecast generation, a subset of the OSI SAF (1978-present) and ERA5 datasets (1940-present) are utilised for training.\n",
    "\n",
    "For more details, the [original paper](https://www.nature.com/articles/s41467-021-25257-4) is a recommended read. A more up-to-date paper considering the latest IceNet implementation changes since the publication of the original paper is currently in progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended usage\n",
    "\n",
    "This has been a very simplified demonstrator notebook that covers the usage of IceNet only via the Python API. There are also command line interface and bash scripts that allow operationalising IceNet and simplify the typical workflows one might come across when generating sea ice forecasts.\n",
    "\n",
    "To learn more about such approaches, there are a series of introductory notebooks (five notebooks at present) that cover both the fundamental and advanced usage scenarios of the IceNet library under the [icenet-notebooks](https://github.com/icenet-ai/icenet-notebooks) repository. They are as follows:\n",
    "\n",
    "* [Introductory usage via CLI](https://github.com/icenet-ai/icenet-notebooks/blob/main/01.cli_demonstration.ipynb): Demonstrates the usage of the different stages of icenet (download, process, train, predict and output) via a very high level command line interface.\n",
    "* [Pipeline usage](https://github.com/icenet-ai/icenet-notebooks/blob/main/02.pipeline_demonstration.ipynb): Creating operational and reproducible end-to-end runs.\n",
    "* [Data structure and analysis](https://github.com/icenet-ai/icenet-notebooks/blob/main/03.data_and_forecasts.ipynb): Understand the structure of the data stores and products created by these workflows and what tools currently exist in IceNet to look over them.\n",
    "* [Library usage](https://github.com/icenet-ai/icenet-notebooks/blob/main/04.library_usage.ipynb): Understand how to programmatically perform an end to end run.\n",
    "* [Library extension](https://github.com/icenet-ai/icenet-notebooks/blob/main/05.library_extension.ipynb): Understand why and how to extend the IceNet library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Citing this Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "Please see [CITATION.cff](https://github.com/eds-book/67a1e320-7c47-4ea9-8df8-e868326bc90b/blob/main/CITATION.cff) for the full citation information. The citation file can be exported to APA or BibTex formats (learn more [here](https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/about-citation-files))."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional information\n",
    "\n",
    "**Review**: This notebook has been reviewed by one or more members of the Environmental Data Science book community. The open review is available [here](https://github.com/alan-turing-institute/environmental-ds-book/issues/239).\n",
    "\n",
    "**Codebase**: [IceNet v0.2.9a1](https://github.com/icenet-ai/icenet/tree/0bac48fbc6447d937498b2c7240a2bd946430700).\n",
    "\n",
    "**License**: The code in this notebook is licensed under the MIT License. The Environmental Data Science book is licensed under the Creative Commons by Attribution 4.0 license. See further details [here](https://github.com/eds-book-gallery/67a1e320-7c47-4ea9-8df8-e868326bc90b/blob/main/LICENSE).\n",
    "\n",
    "**Contact**: If you have any suggestion or report an issue with this notebook, feel free to [create an issue](https://github.com/alan-turing-institute/environmental-ds-book/issues/new/choose) or send a direct message to [environmental.ds.book@gmail.com](mailto:environmental.ds.book@gmail.com)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import icenet\n",
    "icenet_version = icenet.__version__\n",
    "print(f'IceNet version: {icenet_version}')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "from datetime import date\n",
    "\n",
    "print('Notebook repository version: v2.0.0')\n",
    "print(f'Last tested: {date.today()}')"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
